# Logistic Regression
### 1. Import Libraries
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    import matplotlib.pyplot as plt
### 2. Identify Feature and Target Variables
    X = df.drop(columns='target_column_name') # features
    y = df['target_column_name'] # target
### 3. Split Data into Train and Test Sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
### 4. Process Data
    scaler = StandardScaler() # initiate StandardScaler
### 5. Train Data
    X_train_scaled = scaler.fit_transform(X_train) # fit and transform train data
    X_test_scaled = scaler.transform(X_test) # transform test data
### 6. Run Linear Regression Model
    log_reg = LogisticRegression() # initiate regression model
    log_reg.fit(X_train_scaled, y_train) # train model
    y_pred = log_reg.predict(X_test_scaled) # predict data
### 7. Fine Tuning
    log_reg_tuned = LogisticRegression(max_iter=200, penalty='l2', C=1.0, solver='lbfgs')
    log_reg_tuned.fit(X_train_scaled, y_train)
    y_pred_tuned = log_reg_tuned.predict(X_test_scaled)
* **max_iter:** maximum number of iterations.
* **penalty:** defines type of regularization. Options: l1, l2, elastic, none. l2 is default.
* **C:** controls the strength of regularization. Lower values (c=0.1) apply stronger regularization (reduce model complexity, prevent overfitting). Higher values (c=100) apply less regularization (capture more complex data, prone to overfitting).
* **solver:** defines the algorithm used for search. Options: lbfgs, liblinear, sag, saga. Default is lbfgs.
### 8. Hyperparameter Tuning
    parameter_grid = { # define hyperparameter
        'C': [0.1, 1, 10, 100],
        'penalty': ['l2'],
        'solver': ['lbfgs', 'liblinear'],
        'max_iter': [100, 200, 300]
    }
    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy') # initiate gridsearch cv
    grid_search.fit(X_train_scaled, y_train) # train model
    print("Best Hyperparameters: ", grid_search.best_params_) # identify best hyperparameter
    best_model = grid_search.best_estimator_ # run model with best hyperparameter
    y_pred_best = best_model.predict(X_test_scaled) # predict values
### 9. Evaluate Model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
- **Mean Squared Error (MSE):** The closer to 0, the lower the deviation between true value and prediction (= good fit). MSE is sensitive to outliert!
- **R2-Score:** R2=1: perfect fit, R2=<0: model does not explain the variance. Example: R2=.8 means that 80% of the variance is explained by the model.
### 10. Visualization
    plt.scatter(X_test, y_test, color='blue', label='data')
    plt.scatter(X_test, y_pred, color='orange', label='predicted values')
    plt.plot(X_test, y_pred, color='gray', label='regression line')
    plt.xlabel('Ffeature')
    plt.ylabel('target')
    plt.legend()
    plt.show()
- **Blue points (true data):** scattered along a certain trend
- **Orange points (predicted values):** that should closely follow the blue points if the model is good
- **Gray regression line:** represents the linear modelâ€™s prediction of how y changes with X
### 11. Feature Importance Analysis
    coefficients = model.coef_
    features = X.columns
    importance_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
    importance_df['Absolute Coefficient'] = importance_df['Coefficient'].abs()
    importance_df = importance_df.sort_values(by='Absolute Coefficient', ascending=False)
    print(importance_df)
- Features with higher absolute coefficients are more important
