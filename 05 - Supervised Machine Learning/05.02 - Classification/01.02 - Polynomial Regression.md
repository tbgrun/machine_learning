# Polynomial Regression
### 1. Import Libraries
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import PolynomialFeatures, StandardScaler
    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElsaticNet
    from sklearn.pipeline import Pipeline
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.inspection import permutation_importance
    import matplotlib.pyplot as plt
### 2. Identify Feature and Target Variables
    X = df.drop(columns='target_column_name') # features
    y = df['target_column_name'] # target
### 3. Split Data into Train and Test Sets
[klick here to get to "Data Splitting"](https://github.com/tbgrun/machine_learning/blob/main/03%20-%20Data%20Splitting/00%20-%20Data%20Splitting.md)
### 4. Build Pipeline for Linear Regression with Polynomial Features
    pipeline = Pipeline([
        ('poly_features', PolynomialFeatures(degree=2)),
        ('scaler', StandardScaler()),
        ('regressor', LogisticRegression())
    ])
More Information
* [Polynomial Features](https://github.com/tbgrun/machine_learning/blob/main/02%20-%20Data%20Wrangling/08%20-%20Feature%20Engineering.md#12-polynomial-features)
* [Scaling](https://github.com/tbgrun/machine_learning/blob/main/02%20-%20Data%20Wrangling/10%20-%20Scaling.md)
* [Linear Regression](https://github.com/tbgrun/machine_learning/blob/main/05%20-%20Supervised%20Machine%20Learning/01.01%20-%20Linear%20Regression.md)
### 5. Fine Tuning
#### 5.1 Manual Fine Tuning
##### 5.1.1 Fine Tune
    log_reg_tuned = LogisticRegression(max_iter=200, penalty='l2', C=1.0, solver='lbfgs')
    log_reg_tuned.fit(X_train_scaled, y_train)
* **max_iter:** maximum number of iterations.
* **penalty:** defines type of regularization. Options: l1, l2, elastic, none. l2 is default.
* **C:** controls the strength of regularization. Lower values (c=0.1) apply stronger regularization (reduce model complexity, prevent overfitting). Higher values (c=100) apply less regularization (capture more complex data, prone to overfitting).
* **solver:** defines the algorithm used for search. Options: lbfgs, liblinear, sag, saga. Default is lbfgs.
##### 5.1.2 Evaluation
[klick here to get to "Performance Metrics Overview"](https://github.com/tbgrun/machine_learning/blob/main/99%20-%20Supplementary%20Materials/01%20-%20Performance%20Metrics%20Overview.md)

    y_pred_tuned = log_reg_tuned.predict(X_test_scaled)
    print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred_tuned)}")
    print(f"Classification Report:\n{classification_report(y_test, y_pred_tuned)}")
    y_prob_tuned = log_reg_tuned.predict_proba(X_test_scaled)[:, 1]  # Probability of the positive class
    roc_auc_tuned = roc_auc_score(y_test, y_prob_tuned)
    print(f"ROC AUC Score: {roc_auc_tuned}")
##### 5.1.3 Visualization
    plt.scatter(y_test, y_pred_tuned)
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='orange', linestyle='--')  # baseline
    plt.show()
#### 5.2 Automated Hyperparameter Tuning
##### 5.2.1 Setup GridSearchCV
    parameter_grid = [
         {
             'poly_features__degree': [2, 3, 4],  # different degrees
             'classifier__C': [0.1, 1.0, 10.0]  # regularization strength
         },
     ]
* **classifier__C:** controls the strength of regularization. Lower values (c=0.1) apply stronger regularization (reduce model complexity, prevent overfitting). Higher values (c=100) apply less regularization (capture more complex data, prone to overfitting).
##### 5.2.2 Run Hyperparameter Tuning
    grid_search = GridSearchCV(pipeline, parameter_grid, cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train) # fit model
    best_model = grid_search.best_estimator_
##### 5.2.3 Evaluation
[klick here to get to "Performance Metrics Overview"](https://github.com/tbgrun/machine_learning/blob/main/99%20-%20Supplementary%20Materials/01%20-%20Performance%20Metrics%20Overview.md)

    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
###### 5.2.3.1 Evaluation Metrics
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:\n", cm)
    class_report = classification_report(y_test, y_pred)
    print("Classification Report:\n", class_report)
    y_pred_prob = best_model.predict_proba(X_test)[:, 1]
    roc_auc = roc_auc_score(y_test, y_pred_prob)  
###### 5.2.3.1 Visualization
    plt.scatter(y_test, y_pred)
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='orange', linestyle='--')  # baseline
    plt.show()
### 6. Feature Importance Analysis
    result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42)
    feature_importances = result.importances_mean
    for i, importance in enumerate(feature_importances):
        print(f"Feature {i}: Importance {importance}")
