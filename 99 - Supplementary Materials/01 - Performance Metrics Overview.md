# Performance Metrics
* **Accuracy:** Measures the proportion of correct predictions out of all predictions made.
* **Precision:** Measures the proportion of true positives out of all positive predictions.
* **Recall** (Sensitivity or True Positive Rate): Measures the proportion of actual positives that are correctly identified by the model.
* **F1 Score:** The harmonic mean of precision and recall, providing a balance between the two.
* **Squared Error:** This is a type of error metric that measures the difference between the predicted value and the actual value. The error is squared to emphasize larger errors and to avoid canceling out positive and negative errors. It’s commonly used in regression tasks. The formula is: Squared Error=(ytrue−ypredicted)2
* **Mean Squared Error (MSE):** The average of the squared errors, commonly used in regression.
* **ROC** (Receiver Operating Characteristic): A graphical plot used to evaluate the diagnostic ability of a binary classifier. The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings.
* **AUC** (Area Under the ROC Curve): Summarizes the performance of a binary classification model by calculating the area under the ROC curve.
